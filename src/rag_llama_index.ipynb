{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d34c698-9308-4704-bd72-805881780d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45785014-c220-48fd-a3d0-95d9b44a31a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9119139-0301-46b3-a907-4af52f58b61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input a directory to be scanned:  Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: dc7140c6-5a33-4945-8481-62d3536f2f59\n",
      "Text: The Union Budget 2024, presented by Finance Minister Nirmala\n",
      "Sitharaman, has set the stage for a transformative period in India's\n",
      "economic landscape. With a focus on education, employment, urban\n",
      "development, and technological advancement, the budget aims to propel\n",
      "India towards sustainable growth and prosperity.  The Union Budget\n",
      "2024, presented...\n"
     ]
    }
   ],
   "source": [
    "documentDirectory = input(\"Input a directory to be scanned: \")\n",
    "formattedPath = Path(documentDirectory)\n",
    "formattedPath\n",
    "documents = SimpleDirectoryReader(formattedPath).load_data()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189bbdff-1d05-41c6-8cf9-3ef30f0b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=250, chunk_overlap=10)\n",
    "nodes = text_splitter.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc7851c-d21b-4f33-a27e-b90f89fa966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "collection_name = 'BudgetArticle1'\n",
    "if collection_name in [coll.name for coll in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "chroma_collection = chroma_client.create_collection(collection_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07a056b-9f4e-4282-a2ca-ced5afe74fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 0fc845d8-b888-478b-b33d-105c52830d3f\n",
      "Insert of existing embedding ID: ddaedf2b-cad0-48c8-8fa4-c055110105fe\n",
      "Insert of existing embedding ID: e9e30c35-bafe-4ce2-b675-c3f39abfd694\n",
      "Insert of existing embedding ID: 9b548a18-8721-4d98-955f-afd8c95c60dd\n",
      "Insert of existing embedding ID: c5c0443d-3fc7-4fbe-8473-ca14c0fac5cc\n",
      "Insert of existing embedding ID: 29cfadd5-7017-48ad-9e3a-e2579aa2809f\n",
      "Insert of existing embedding ID: 5a752e2e-c286-4895-8fa1-80855cefcb95\n",
      "Insert of existing embedding ID: 7310cc76-7b54-4f2d-897f-71e496c757f4\n",
      "Insert of existing embedding ID: f77d442a-b4a9-4c7f-abba-fe091e00890d\n",
      "Insert of existing embedding ID: 85ef20b2-5fc5-45ce-bd36-e22080e47607\n",
      "Insert of existing embedding ID: a4570531-ed10-4d48-9f7c-6b85fc2316c7\n",
      "Insert of existing embedding ID: e4f40c53-c2c3-4bd3-b380-62ca5620f407\n",
      "Insert of existing embedding ID: 19e5eb74-58eb-4df0-ba4f-e42cc2e962ce\n",
      "Insert of existing embedding ID: 44dc3bb9-04c5-4b72-a12b-1f611591d1df\n",
      "Insert of existing embedding ID: cb6b375f-512d-4504-95e4-7967c6e40529\n",
      "Insert of existing embedding ID: bdc66296-f066-4ccc-b967-1710fcb999a0\n",
      "Insert of existing embedding ID: f5c1ef16-2a62-4b0f-97c6-a0f5fb726bce\n",
      "Add of existing embedding ID: 0fc845d8-b888-478b-b33d-105c52830d3f\n",
      "Add of existing embedding ID: ddaedf2b-cad0-48c8-8fa4-c055110105fe\n",
      "Add of existing embedding ID: e9e30c35-bafe-4ce2-b675-c3f39abfd694\n",
      "Add of existing embedding ID: 9b548a18-8721-4d98-955f-afd8c95c60dd\n",
      "Add of existing embedding ID: c5c0443d-3fc7-4fbe-8473-ca14c0fac5cc\n",
      "Add of existing embedding ID: 29cfadd5-7017-48ad-9e3a-e2579aa2809f\n",
      "Add of existing embedding ID: 5a752e2e-c286-4895-8fa1-80855cefcb95\n",
      "Add of existing embedding ID: 7310cc76-7b54-4f2d-897f-71e496c757f4\n",
      "Add of existing embedding ID: f77d442a-b4a9-4c7f-abba-fe091e00890d\n",
      "Add of existing embedding ID: 85ef20b2-5fc5-45ce-bd36-e22080e47607\n",
      "Add of existing embedding ID: a4570531-ed10-4d48-9f7c-6b85fc2316c7\n",
      "Add of existing embedding ID: e4f40c53-c2c3-4bd3-b380-62ca5620f407\n",
      "Add of existing embedding ID: 19e5eb74-58eb-4df0-ba4f-e42cc2e962ce\n",
      "Add of existing embedding ID: 44dc3bb9-04c5-4b72-a12b-1f611591d1df\n",
      "Add of existing embedding ID: cb6b375f-512d-4504-95e4-7967c6e40529\n",
      "Add of existing embedding ID: bdc66296-f066-4ccc-b967-1710fcb999a0\n",
      "Add of existing embedding ID: f5c1ef16-2a62-4b0f-97c6-a0f5fb726bce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x000002C4ED01C110>\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(nodes=nodes, storage_context=storage_context, embed_model=OpenAIEmbedding())\n",
    "#full_path = os.path.join(formattedPath, documentDirectory+'_index.pkl')\n",
    "#with open(full_path, 'wb') as file:\n",
    "#    pickle.dump(index, file)\n",
    "\n",
    "#with open(full_path, 'rb') as file:\n",
    "#    index = pickle.load(file)\n",
    "#    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f090798-ddef-4d01-a630-228f83d254f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question:  How is budget\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChromaVectorStore' object has no attribute '_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your question: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m retriever \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\base\\base_retriever.py:243\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    240\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[0;32m    241\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    242\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[1;32m--> 243\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[0;32m    245\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    246\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[0;32m    247\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:101\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     96\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     97\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mget_agg_embedding_from_queries(\n\u001b[0;32m     98\u001b[0m                 query_bundle\u001b[38;5;241m.\u001b[39membedding_strs\n\u001b[0;32m     99\u001b[0m             )\n\u001b[0;32m    100\u001b[0m         )\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:177\u001b[0m, in \u001b[0;36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[1;34m(self, query_bundle_with_embeddings)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_nodes_with_embeddings\u001b[39m(\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[0;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    176\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_vector_store_query(query_bundle_with_embeddings)\n\u001b[1;32m--> 177\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_node_list_from_query_result(query_result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:371\u001b[0m, in \u001b[0;36mChromaVectorStore.query\u001b[1;34m(self, query, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query\u001b[38;5;241m.\u001b[39mquery_embedding:\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(limit\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39msimilarity_top_k, where\u001b[38;5;241m=\u001b[39mwhere, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:381\u001b[0m, in \u001b[0;36mChromaVectorStore._query\u001b[1;34m(self, query_embeddings, n_results, where, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m, query_embeddings: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_results: \u001b[38;5;28mint\u001b[39m, where: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    380\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VectorStoreQueryResult:\n\u001b[1;32m--> 381\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m    382\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[0;32m    383\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[0;32m    384\u001b[0m         where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    386\u001b[0m     )\n\u001b[0;32m    388\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Top \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    389\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChromaVectorStore' object has no attribute '_collection'"
     ]
    }
   ],
   "source": [
    "#question = \"Explain the article in brief\"\n",
    "question = input(\"Enter your question: \")\n",
    "retriever = index.as_retriever()\n",
    "retriever.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f9a37f2-a9c8-41f3-b8af-e3345f66cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to provide information on the weather as it is not mentioned in the context provided.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8f3f93-b1c9-43f8-8fd0-ec8b9b852095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response_synthesizer:text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x000001C2ABCCC680>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]), 'response_synthesizer:refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x000001C2ABCCC680>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}\n"
     ]
    }
   ],
   "source": [
    "#This helps in understanding the prompts which query engine uses to give answers\n",
    "prompts = query_engine.get_prompts()\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b8528-c5f5-470b-a977-8c5efd96bc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
